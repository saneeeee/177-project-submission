This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-12-10T04:11:50.323Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
agent/
  prompts.py
  wash_detector.py
generator/
  generator.py
  models.py
utils/
  validation.py
visualization/
  graph.py
eval.py
main.py
visualize_data.py

================================================================
Repository Files
================================================================

================
File: agent/prompts.py
================
from typing import List, Dict
from datetime import datetime
from collections import defaultdict

def prepare_transaction_stats(transactions: List[Dict]) -> Dict:
    """Prepare statistical summaries of transactions"""
    stats = {
        'unique_addresses': set(),
        'address_volumes': defaultdict(float),
        'address_partners': defaultdict(set),
        'time_start': float('inf'),
        'time_end': 0,
        'price_ranges': defaultdict(lambda: {'min': float('inf'), 'max': -float('inf')}),
        'volume_per_hour': defaultdict(float)
    }
    
    for tx in transactions:
        from_addr = tx['from_addr']
        to_addr = tx['to_addr']
        amount = tx['token_amount']
        price = tx['price_eth']
        timestamp = tx['timestamp']
        
        stats['unique_addresses'].add(from_addr)
        stats['unique_addresses'].add(to_addr)
        stats['address_volumes'][from_addr] += amount
        stats['address_volumes'][to_addr] += amount
        stats['address_partners'][from_addr].add(to_addr)
        stats['time_start'] = min(stats['time_start'], timestamp)
        stats['time_end'] = max(stats['time_end'], timestamp)
        stats['price_ranges'][from_addr]['min'] = min(stats['price_ranges'][from_addr]['min'], price)
        stats['price_ranges'][from_addr]['max'] = max(stats['price_ranges'][from_addr]['max'], price)
        
        hour = timestamp - (timestamp % 3600)
        stats['volume_per_hour'][hour] += amount
        
    return stats

def create_analysis_prompt(transactions: List[Dict]) -> str:
    """Create comprehensive analysis prompt with chain-of-thought structure"""
    stats = prepare_transaction_stats(transactions)
    
    # Get top traders by volume
    top_traders = sorted(
        [(addr, vol) for addr, vol in stats['address_volumes'].items()],
        key=lambda x: x[1], 
        reverse=True
    )[:5]

    # Format example transactions
    txs = "\n".join([
        f"Transaction {i+1}:"
        f"\nFrom: {tx['from_addr']}"
        f"\nTo: {tx['to_addr']}"
        f"\nAmount: {tx['token_amount']:.2f}"
        f"\nPrice: {tx['price_eth']:.2f}"
        f"\nTimestamp: {datetime.fromtimestamp(tx['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}"
        for i, tx in enumerate(transactions)
    ])

    return f"""You are a cryptocurrency forensics expert analyzing trading data to detect wash trading. Use the formal definition from academic research to identify wash trading patterns.

KEY WASH TRADING CHARACTERISTICS (from "Detecting and Quantifying Wash Trading on Decentralized Cryptocurrency Exchanges", WWW'21):

1. Legal Definition:
- Trades that "give the appearance that purchases and sales have been made, without incurring market risk or changing the trader's market position"
- Actors executing trades where they end up at the same market position they had initially
- Trades between colluding parties with no real change in ownership

2. Important Distinctions:
- High volume alone is NOT indicative of wash trading
- Market makers legitimately create high volumes through normal market-making activities
- Transaction types in data are labeled as: 'normal', 'market_maker', 'wash', 'camouflage'

3. Key Wash Trading Indicators:
- Circular trading patterns where tokens flow back to originators
- Traders ending up with same positions after series of trades
- Coordinated trading between small groups of addresses
- Suspicious patterns in timing and amounts
- No real risk or position changes

Dataset Overview:
Total Transactions: {len(transactions)}
Unique Addresses: {len(stats['unique_addresses'])}
Time Period: {datetime.fromtimestamp(stats['time_start']).strftime('%Y-%m-%d %H:%M:%S')} to {datetime.fromtimestamp(stats['time_end']).strftime('%Y-%m-%d %H:%M:%S')}

Transactions:
{txs}

Analyze this data following these steps:

1. Network Analysis:
- Look specifically for circular trading patterns
- Identify closed trading loops
- Map token flow patterns
- Distinguish between market making and circular trading

2. Position Analysis:
- Track token positions of trading addresses
- Identify traders returning to initial positions
- Look for zero-sum trading cycles
- Consider normal market maker behavior

3. Temporal Analysis:
- Look for coordinated timing patterns
- Identify suspicious trading frequencies
- Consider natural vs artificial timing
- Account for normal market trading hours

4. Volume Analysis:
- Distinguish between market making and wash trading
- Look for unnatural consistencies in amounts
- Consider normal trading variance
- Identify suspicious patterns while accounting for legitimate high-volume trading

After completing the analysis, provide:

For each step above, provide:
a) Your reasoning process
b) Specific evidence found
d) Confidence in observations

Only after completing all analysis steps, provide your final assessment in this format:

DETAILED REASONING:
[Provide your complete chain of thought]

EVIDENCE SUMMARY:
[List key evidence from each analysis step]

WASH TRADING ASSESSMENT:
[Conclude whether wash trading is likely occurring]

CONFIDENCE LEVEL:
[Provide confidence as percentage and explain why]

SUSPICIOUS ADDRESSES:
[List addresses with suspicious patterns]
"""

================
File: agent/wash_detector.py
================
from typing import List, Dict
import openai
from pydantic import BaseModel
from .prompts import create_analysis_prompt

class WashTradingAnalysis(BaseModel):
    """Structured output from the wash trading analysis"""
    reasoning_process: Dict[str, str]  # Step-by-step reasoning
    evidence: Dict[str, List[str]]     # Evidence per category
    alternative_explanations: List[str]
    is_wash_trading: bool
    confidence: float
    suspicious_addresses: List[str]
    recommendations: List[str]
    raw_response: str

class WashTradeDetector:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)

    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract a specific section from the reasoning text"""
        try:
            return text.split(section_name + ":")[1].split(":")[0].strip()
        except IndexError:
            return ""

    def _parse_evidence_sections(self, evidence_text: str) -> Dict[str, List[str]]:
        """Parse evidence into categorized sections"""
        evidence = {
            'network': [],
            'volume': [],
            'temporal': [],
            'price': [],
            'behavioral': []
        }
        
        current_section = None
        for line in evidence_text.split("\n"):
            line = line.strip()
            if not line:
                continue
                
            if "Network" in line:
                current_section = 'network'
            elif "Volume" in line:
                current_section = 'volume'
            elif "Temporal" in line:
                current_section = 'temporal'
            elif "Price" in line:
                current_section = 'price'
            elif "Behavioral" in line:
                current_section = 'behavioral'
            elif current_section and line.startswith("-"):
                evidence[current_section].append(line[1:].strip())
                
        return evidence

    def _extract_confidence(self, confidence_text: str) -> float:
        """Extract confidence percentage from text"""
        import re
        confidence_match = re.search(r"(\d+)%", confidence_text)
        if confidence_match:
            return float(confidence_match.group(1)) / 100
        return 0.5  # default confidence

    def _parse_response(self, response: str) -> WashTradingAnalysis:
        """Parse the GPT-4 response into structured analysis"""
        sections = {
            'reasoning_process': {},
            'evidence': {},
            'alternative_explanations': [],
            'is_wash_trading': False,
            'confidence': 0.0,
            'suspicious_addresses': [],
            'recommendations': [],
            'raw_response': response
        }

        # Parse reasoning process
        if "DETAILED REASONING:" in response:
            reasoning_text = response.split("DETAILED REASONING:")[1].split("EVIDENCE SUMMARY:")[0]
            sections['reasoning_process'] = {
                'network': self._extract_section(reasoning_text, "Network Analysis"),
                'volume': self._extract_section(reasoning_text, "Volume Analysis"),
                'temporal': self._extract_section(reasoning_text, "Temporal Pattern Analysis"),
                'price': self._extract_section(reasoning_text, "Price Pattern Analysis"),
                'behavioral': self._extract_section(reasoning_text, "Behavioral Analysis")
            }

        # Parse evidence
        if "EVIDENCE SUMMARY:" in response:
            evidence_text = response.split("EVIDENCE SUMMARY:")[1].split("ALTERNATIVE EXPLANATIONS:")[0]
            sections['evidence'] = self._parse_evidence_sections(evidence_text)

        # Parse alternative explanations
        if "ALTERNATIVE EXPLANATIONS:" in response:
            alt_text = response.split("ALTERNATIVE EXPLANATIONS:")[1].split("WASH TRADING ASSESSMENT:")[0]
            sections['alternative_explanations'] = [exp.strip() for exp in alt_text.split("-") if exp.strip()]

        # Parse wash trading assessment
        if "WASH TRADING ASSESSMENT:" in response:
            assessment = response.split("WASH TRADING ASSESSMENT:")[1].split("CONFIDENCE LEVEL:")[0]
            sections['is_wash_trading'] = 'likely' in assessment.lower() or 'confirmed' in assessment.lower()

        # Parse confidence
        if "CONFIDENCE LEVEL:" in response:
            confidence_text = response.split("CONFIDENCE LEVEL:")[1].split("SUSPICIOUS ADDRESSES:")[0]
            sections['confidence'] = self._extract_confidence(confidence_text)

        # Parse suspicious addresses
        if "SUSPICIOUS ADDRESSES:" in response:
            addresses_text = response.split("SUSPICIOUS ADDRESSES:")[1].split("RECOMMENDATIONS:")[0]
            sections['suspicious_addresses'] = [
                addr.strip() for addr in addresses_text.split("\n") 
                if addr.strip() and addr.strip().startswith('r')
            ]

        # Parse recommendations
        if "RECOMMENDATIONS:" in response:
            recommendations_text = response.split("RECOMMENDATIONS:")[1]
            sections['recommendations'] = [
                rec.strip() for rec in recommendations_text.split("-") 
                if rec.strip()
            ]

        return WashTradingAnalysis(**sections)

    def analyze(self, transactions: List[Dict]) -> WashTradingAnalysis:
        """Analyze transactions for wash trading patterns"""
        prompt = create_analysis_prompt(transactions)
        # print()
        # print(prompt)
        # print()
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            analysis = self._parse_response(response.choices[0].message.content)
            return analysis
            
        except Exception as e:
            print(f"Error during analysis: {e}")
            raise

def analyze_wash_trading(transactions: List[Dict], api_key: str) -> WashTradingAnalysis:
    """Convenience function for wash trading analysis"""
    detector = WashTradeDetector(api_key)
    return detector.analyze(transactions)

================
File: generator/generator.py
================
import random
from datetime import datetime
import uuid
from typing import List, Dict, Set
import numpy as np
from collections import defaultdict

from .models import GeneratorParams, WashCamouflageStrategy

class TradingDataGenerator:
    def __init__(self, params: GeneratorParams):
        self.params = params
        random.seed(params.seed)
        np.random.seed(params.seed)
        
        self.addresses = self._generate_addresses()
        self.market_makers = self._setup_market_makers()  # Setup market makers before wash groups
        self.wash_groups = self._setup_wash_groups()
        self.balances = {addr: 1000.0 for addr in self.addresses}
        self.start_time = int(datetime(2024, 1, 1).timestamp())
        
        # track wash trader connections for camouflage
        self.wash_trader_connections = self._setup_wash_trader_connections()
        
    def _generate_deterministic_address(self, index: int) -> str:
        random.seed(self.params.seed + index)
        return 'r' + uuid.UUID(int=random.getrandbits(128)).hex[:39]

    def _generate_addresses(self) -> List[str]:
        addresses = []
        index = 0
        
        # generate addresses for normal traders and market makers
        for _ in range(self.params.num_normal_traders):
            addr = self._generate_deterministic_address(index)
            addresses.append(addr)
            index += 1
        
        # generate addresses for wash traders
        for size in self.params.wash_group_sizes:
            for _ in range(size):
                addr = self._generate_deterministic_address(index)
                addresses.append(addr)
                index += 1
                
        return addresses

    def _setup_market_makers(self) -> Set[str]:
        """Setup market maker addresses from normal traders"""
        market_makers = set()
        if self.params.num_market_makers > 0:
            market_makers = set(self.addresses[:self.params.num_market_makers])
            print("\nMarket Makers:")
            for addr in market_makers:
                print(f"  {addr}")
            print()  # Empty line for formatting
        return market_makers

    def _setup_wash_groups(self) -> List[List[str]]:
        """Create wash trading groups and print their addresses"""
        groups = []
        # Start after normal traders (and market makers)
        start_idx = self.params.num_normal_traders
        
        print("Wash Trading Groups:")
        for group_idx, size in enumerate(self.params.wash_group_sizes):
            group = self.addresses[start_idx:start_idx + size]
            groups.append(group)
            
            print(f"\nGroup {group_idx + 1} (Size: {size}):")
            for addr in group:
                print(f"  {addr}")
                
            start_idx += size
        print()  # Empty line for better formatting
        return groups

    def _setup_wash_trader_connections(self) -> Dict[str, Set[str]]:
        """Setup camouflage connections for wash traders"""
        connections = defaultdict(set)
        if self.params.camouflage.strategy in [WashCamouflageStrategy.RANDOM_CONNECT, 
                                             WashCamouflageStrategy.HYBRID]:
            # Only connect to normal traders that aren't market makers
            normal_traders = set(self.addresses[:self.params.num_normal_traders]) - self.market_makers
            for group in self.wash_groups:
                for washer in group:
                    num_connections = self.params.camouflage.normal_connections_per_washer
                    if num_connections > 0:
                        connections[washer] = set(random.sample(
                            list(normal_traders), 
                            min(num_connections, len(normal_traders))
                        ))
        return connections

    def _generate_camouflage_transactions(self, wash_trader: str, timestamp: int) -> List[Dict]:
        """Generate camouflage transactions for a wash trader"""
        transactions = []
        
        if random.random() < self.params.camouflage.normal_trade_probability:
            connected_traders = self.wash_trader_connections[wash_trader]
            if connected_traders:
                trader = random.choice(list(connected_traders))
                # decide direction
                if random.random() < 0.5:
                    from_addr, to_addr = wash_trader, trader
                else:
                    from_addr, to_addr = trader, wash_trader
                
                # generate amount similar to normal trading patterns
                amount = np.random.normal(
                    self.params.normal_trade_size_mean,
                    self.params.normal_trade_size_std
                )
                amount = max(0.1, amount)  # Ensure positive amount
                
                tx = {
                    'tx_id': str(uuid.UUID(int=random.getrandbits(128))),
                    'timestamp': timestamp,
                    'from_addr': from_addr,
                    'to_addr': to_addr,
                    'token_amount': amount,
                    'token_symbol': self.params.token_symbol,
                    'price_eth': self._generate_price(),
                    'tx_type': 'camouflage'
                }
                transactions.append(tx)
        
        return transactions

    def _generate_price(self) -> float:
        """Generate price with volatility"""
        return 1.0 * (1 + random.uniform(-self.params.price_volatility, 
                                       self.params.price_volatility))

    def generate_transactions(self) -> List[dict]:
        transactions = []
        time_span = self.params.time_span_days * 24 * 3600
        
        random.seed(self.params.seed)
        np.random.seed(self.params.seed)

        # generate wash trading transactions
        for group_idx, group in enumerate(self.wash_groups):
            wash_amount = self.params.wash_amounts[group_idx]
            num_cycles = self.params.wash_tx_counts[group_idx]
            
            for cycle in range(num_cycles):
                base_timestamp = self.start_time + (cycle * time_span // num_cycles)
                
                # add temporal variance if using temporal camouflage
                if self.params.camouflage.strategy in [WashCamouflageStrategy.TEMPORAL, 
                                                     WashCamouflageStrategy.HYBRID]:
                    base_timestamp += random.randint(
                        int(self.params.camouflage.min_time_between_wash * 3600),
                        int(time_span / num_cycles)
                    )
                
                group_order = list(group)
                random.Random(self.params.seed + group_idx + cycle).shuffle(group_order)
                
                # generate wash cycle
                for i in range(len(group_order)):
                    from_addr = group_order[i]
                    to_addr = group_order[(i + 1) % len(group_order)]
                    
                    # add amount variance if using volume-based camouflage
                    actual_amount = wash_amount
                    if self.params.camouflage.strategy in [WashCamouflageStrategy.VOLUME_BASED, 
                                                         WashCamouflageStrategy.HYBRID]:
                        variance = wash_amount * self.params.camouflage.wash_amount_variance
                        actual_amount += random.uniform(-variance, variance)
                    
                    tx = {
                        'tx_id': str(uuid.UUID(int=random.getrandbits(128))),
                        'timestamp': base_timestamp + i * 3600,
                        'from_addr': from_addr,
                        'to_addr': to_addr,
                        'token_amount': actual_amount,
                        'token_symbol': self.params.token_symbol,
                        'price_eth': self._generate_price(),
                        'tx_type': 'wash'
                    }
                    transactions.append(tx)
                    
                    # generate camouflage transactions
                    transactions.extend(self._generate_camouflage_transactions(
                        from_addr, 
                        base_timestamp + i * 3600
                    ))

        # generate normal trading transactions
        remaining_tx = self.params.num_transactions - len(transactions)
        normal_traders = set(self.addresses[:self.params.num_normal_traders])
        non_market_makers = normal_traders - self.market_makers
        
        for i in range(remaining_tx):
            timestamp = self.start_time + (i * time_span // remaining_tx)
            
            # prefer market makers as counterparties based on activeness
            if self.market_makers and random.random() < self.params.market_maker_activeness:
                if random.random() < 0.5:
                    from_addr = random.choice(list(self.market_makers))
                    to_addr = random.choice(list(non_market_makers))
                else:
                    from_addr = random.choice(list(non_market_makers))
                    to_addr = random.choice(list(self.market_makers))
                tx_type = 'market_maker'
            else:
                from_addr = random.choice(list(normal_traders))
                to_addr = random.choice(list(normal_traders - {from_addr}))
                tx_type = 'normal'
            
            amount = max(0.1, np.random.normal(
                self.params.normal_trade_size_mean,
                self.params.normal_trade_size_std
            ))
            
            tx = {
                'tx_id': str(uuid.UUID(int=random.getrandbits(128))),
                'timestamp': timestamp,
                'from_addr': from_addr,
                'to_addr': to_addr,
                'token_amount': amount,
                'token_symbol': self.params.token_symbol,
                'price_eth': self._generate_price(),
                'tx_type': tx_type
            }
            transactions.append(tx)

        transactions.sort(key=lambda x: x['timestamp'])
        return transactions

================
File: generator/models.py
================
from pydantic import BaseModel, Field, validator
from typing import Tuple, Optional
from enum import Enum

class WashCamouflageStrategy(str, Enum):
    NONE = "none"  # No camouflage
    RANDOM_CONNECT = "random_connect"  # Random connections to normal traders
    VOLUME_BASED = "volume_based"  # Mix wash volumes with normal trading volumes
    TEMPORAL = "temporal"  # Spread wash trades temporally among normal trades
    HYBRID = "hybrid"  # Combination of multiple strategies

class WashCamouflageParams(BaseModel):
    strategy: WashCamouflageStrategy = Field(default=WashCamouflageStrategy.NONE)
    # Probability of a wash trader engaging in normal trading
    normal_trade_probability: float = Field(default=0.0, ge=0.0, le=1.0)
    # What fraction of wash trading volume goes to normal trades
    volume_leakage: float = Field(default=0.0, ge=0.0, le=1.0)
    # How many normal traders each wash trader connects to
    normal_connections_per_washer: int = Field(default=0, ge=0)
    # How much to vary wash amounts to look like normal trades
    wash_amount_variance: float = Field(default=0.0, ge=0.0, le=1.0)
    # Minimum time between wash trades (in hours)
    min_time_between_wash: float = Field(default=0.0, ge=0.0)
    
class GeneratorParams(BaseModel):
    seed: int = Field(..., description="Random seed for reproducibility")
    num_transactions: int = Field(..., ge=1)
    num_normal_traders: int = Field(..., ge=2)
    num_wash_groups: int = Field(..., ge=0)
    wash_group_sizes: Tuple[int, ...] = Field(default=())
    wash_amounts: Tuple[float, ...] = Field(default=())
    wash_tx_counts: Tuple[int, ...] = Field(default=())
    time_span_days: int = Field(..., ge=1)
    token_symbol: str = Field(default="TOKEN1")  # TODO 0.1: single token assumption
    price_volatility: float = Field(default=0.2, ge=0.0, le=1.0)
    
    # New parameters for normal trading patterns
    normal_trade_size_mean: float = Field(default=50.0, ge=0.0)
    normal_trade_size_std: float = Field(default=20.0, ge=0.0)
    normal_trade_frequency_mean: float = Field(default=24.0, ge=0.0)  # hours
    normal_trade_frequency_std: float = Field(default=12.0, ge=0.0)   # hours
    
    # Camouflage parameters
    camouflage: WashCamouflageParams = Field(default_factory=WashCamouflageParams)
    
    # Market maker simulation
    num_market_makers: int = Field(default=0, ge=0)
    market_maker_activeness: float = Field(default=0.5, ge=0.0, le=1.0)

    @validator('wash_group_sizes', 'wash_amounts', 'wash_tx_counts')
    def validate_wash_params(cls, v, values):
        if 'num_wash_groups' in values:
            if len(v) != values['num_wash_groups']:
                raise ValueError(
                    f'Length of wash parameters must match num_wash_groups: {values["num_wash_groups"]}'
                )
        return v

    @validator('num_market_makers')
    def validate_market_makers(cls, v, values):
        if v > 0 and 'num_normal_traders' in values:
            if v >= values['num_normal_traders']:
                raise ValueError('Number of market makers must be less than number of normal traders')
        return v

================
File: utils/validation.py
================
from typing import List, Dict

def validate_transactions(transactions: List[dict], initial_balance: float = 1000.0) -> bool:
    """Validate transaction sequence for balance consistency"""
    # Get unique addresses
    addresses = set()
    for tx in transactions:
        addresses.add(tx['from_addr'])
        addresses.add(tx['to_addr'])
    
    # Initialize balances
    balances = {addr: initial_balance for addr in addresses}
    
    for tx in transactions:
        amount = tx['token_amount']
        
        if balances[tx['from_addr']] < amount:
            return False
            
        balances[tx['from_addr']] -= amount
        balances[tx['to_addr']] += amount
        
    return True

================
File: visualization/graph.py
================
from pyvis.network import Network
from typing import List, Dict
from collections import defaultdict
import webbrowser
import os
import tempfile


def create_transaction_graph(transactions: List[Dict], output_path: str = None) -> str:
    """
    Create an interactive visualization of transaction patterns.
    Returns the path to the generated HTML file.
    """
    # Create network
    net = Network(notebook=False, height="750px", width="100%", bgcolor="#ffffff", 
                 font_color="#000000")
    net.force_atlas_2based()
    
    # Track total volume between addresses
    edge_volumes = defaultdict(float)
    for tx in transactions:
        key = (tx['from_addr'], tx['to_addr'])
        edge_volumes[key] += tx['token_amount']

    # Track total volume per address
    address_volumes = defaultdict(float)
    for tx in transactions:
        address_volumes[tx['from_addr']] += tx['token_amount']
        address_volumes[tx['to_addr']] += tx['token_amount']

    # Add nodes
    addresses = set()
    for tx in transactions:
        addresses.add(tx['from_addr'])
        addresses.add(tx['to_addr'])

    for addr in addresses:
        # Use shortened address for display
        label = f"{addr[:6]}...{addr[-4:]}"
        volume = address_volumes[addr]
        # Scale node size based on volume
        size = min(50, 10 + (volume / max(address_volumes.values())) * 40)
        net.add_node(addr, label=label, title=f"Volume: {volume:.2f}", size=size)

    # Add edges
    for (from_addr, to_addr), volume in edge_volumes.items():
        # Scale edge width based on volume
        width = 1 + (volume / max(edge_volumes.values())) * 5
        net.add_edge(from_addr, to_addr, value=width, 
                    title=f"Volume: {volume:.2f}",
                    color='#0000FF')

    # Save and open
    if output_path is None:
        # Create temporary file
        temp_dir = tempfile.gettempdir()
        output_path = os.path.join(temp_dir, "transaction_graph.html")
    
    net.save_graph(output_path)
    webbrowser.open('file://' + os.path.abspath(output_path))
    
    return output_path

================
File: eval.py
================
# eval.py
import concurrent.futures
from dataclasses import dataclass
from enum import Enum
import json
from pathlib import Path
import time
from typing import List, Tuple, Dict, Set
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import numpy as np
from dotenv import load_dotenv
import os
import logging
from datetime import datetime

from washdetector.generator.models import GeneratorParams, WashCamouflageParams, WashCamouflageStrategy
from washdetector.generator.generator import TradingDataGenerator
from washdetector.utils.validation import validate_transactions
from washdetector.visualization.graph import create_transaction_graph
from washdetector.agent.wash_detector import analyze_wash_trading

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Difficulty(Enum):
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"

@dataclass
class EvaluationMetrics:
    precision: float
    recall: float
    f1_score: float
    true_positives: int
    false_positives: int
    false_negatives: int
    true_negatives: int
    execution_time: float
    ground_truth_info: Dict
    detection_info: Dict

@dataclass
class TestCase:
    difficulty: Difficulty
    params: GeneratorParams
    seed: int

def setup_output_directory() -> Path:
    """Create timestamped output directory"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(f"evaluation_results_{timestamp}")
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir

def get_params_by_difficulty(difficulty: Difficulty, seed: int) -> GeneratorParams:
    """Generate parameters based on difficulty level"""
    base_params = {
        'seed': seed,
        'token_symbol': 'TEST',
        'price_volatility': 0.1,
        'normal_trade_size_mean': 15.0,
        'normal_trade_size_std': 5.0,
    }
    
    if difficulty == Difficulty.EASY:
        return GeneratorParams(
            **base_params,
            num_transactions=300,
            num_normal_traders=100,
            num_wash_groups=1,
            wash_group_sizes=(4,),
            wash_amounts=(20.0,),
            wash_tx_counts=(30,),
            time_span_days=30,
        )
    elif difficulty == Difficulty.MEDIUM:
        return GeneratorParams(
            **base_params,
            num_transactions=500,
            num_normal_traders=100,
            num_wash_groups=1,
            wash_group_sizes=(4,),
            wash_amounts=(20.0,),
            wash_tx_counts=(30,),
            time_span_days=30,
            num_market_makers=5,
            market_maker_activeness=0.6,
        )
    else:  # HARD
        return GeneratorParams(
            **base_params,
            num_transactions=1000,
            num_normal_traders=100,
            num_wash_groups=2,
            wash_group_sizes=(4, 7),
            wash_amounts=(20.0, 10.0),
            wash_tx_counts=(30, 20),
            time_span_days=30,
            num_market_makers=5,
            market_maker_activeness=0.6,
            camouflage=WashCamouflageParams(
                strategy=WashCamouflageStrategy.HYBRID,
                normal_trade_probability=0.3,
                volume_leakage=0.2,
                normal_connections_per_washer=5,
                wash_amount_variance=0.3,
                min_time_between_wash=1.0
            )
        )

def generate_test_cases(num_cases_per_difficulty: int) -> List[TestCase]:
    """Generate test cases for each difficulty level"""
    test_cases = []
    base_seed = 42

    for difficulty in Difficulty:
        for i in range(num_cases_per_difficulty):
            seed = base_seed + (i * 100) + (list(Difficulty).index(difficulty) * 1000)
            params = get_params_by_difficulty(difficulty, seed)
            test_cases.append(TestCase(difficulty, params, seed))
    
    return test_cases

def compute_metrics(ground_truth: Set[str], predicted: Set[str], all_addresses: Set[str], 
                   execution_time: float, ground_truth_info: Dict, detection_info: Dict) -> EvaluationMetrics:
    """Compute evaluation metrics comparing ground truth to predicted wash traders"""
    true_positives = len(ground_truth.intersection(predicted))
    false_positives = len(predicted - ground_truth)
    false_negatives = len(ground_truth - predicted)
    true_negatives = len(all_addresses - ground_truth - predicted)
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return EvaluationMetrics(
        precision=precision,
        recall=recall,
        f1_score=f1_score,
        true_positives=true_positives,
        false_positives=false_positives,
        false_negatives=false_negatives,
        true_negatives=true_negatives,
        execution_time=execution_time,
        ground_truth_info=ground_truth_info,
        detection_info=detection_info
    )

def plot_metrics(results: List[Dict], output_dir: Path):
    """Generate and save visualization plots"""
    # Prepare data
    df = pd.DataFrame(results)
    
    # Plot 1: Precision, Recall, F1 by Difficulty
    plt.figure(figsize=(12, 6))
    metrics = ['precision', 'recall', 'f1_score']
    
    df_melted = df.melt(
        id_vars=['difficulty'],
        value_vars=metrics,
        var_name='Metric',
        value_name='Score'
    )
    
    sns.barplot(data=df_melted, x='difficulty', y='Score', hue='Metric')
    plt.title('Detection Performance by Difficulty')
    plt.tight_layout()
    plt.savefig(output_dir / 'performance_by_difficulty.png')
    plt.close()
    
    # Plot 2: Confusion Matrices
    for difficulty in df['difficulty'].unique():
        diff_data = df[df['difficulty'] == difficulty]
        conf_matrix = np.array([
            [diff_data['true_negatives'].mean(), diff_data['false_positives'].mean()],
            [diff_data['false_negatives'].mean(), diff_data['true_positives'].mean()]
        ])
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            conf_matrix,
            annot=True,
            fmt='.2f',
            cmap='Blues',
            xticklabels=['Normal', 'Wash Trade'],
            yticklabels=['Normal', 'Wash Trade']
        )
        plt.title(f'Average Confusion Matrix - {difficulty} Difficulty')
        plt.tight_layout()
        plt.savefig(output_dir / f'confusion_matrix_{difficulty}.png')
        plt.close()
    
    # Plot 3: Performance vs Time
    plt.figure(figsize=(10, 6))
    for difficulty in df['difficulty'].unique():
        diff_data = df[df['difficulty'] == difficulty]
        plt.scatter(
            diff_data['execution_time'],
            diff_data['f1_score'],
            label=difficulty,
            alpha=0.7
        )
    
    plt.xlabel('Execution Time (seconds)')
    plt.ylabel('F1 Score')
    plt.title('Detection Performance vs Execution Time')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'performance_vs_time.png')
    plt.close()
    
    # Plot 4: Volume Distribution
    plt.figure(figsize=(10, 6))
    volume_data = []
    for result in results:
        gt_info = result['ground_truth_info']
        volume_data.append({
            'difficulty': result['difficulty'],
            'wash_volume_pct': gt_info['metrics']['wash_volume_percentage'],
            'normal_volume_pct': 1 - gt_info['metrics']['wash_volume_percentage']
        })
    
    volume_df = pd.DataFrame(volume_data)
    volume_df_melted = volume_df.melt(
        id_vars=['difficulty'],
        var_name='Volume Type',
        value_name='Percentage'
    )
    
    sns.boxplot(data=volume_df_melted, x='difficulty', y='Percentage', hue='Volume Type')
    plt.title('Volume Distribution by Difficulty')
    plt.tight_layout()
    plt.savefig(output_dir / 'volume_distribution.png')
    plt.close()

def run_single_evaluation(test_case: TestCase, api_key: str) -> Dict:
    """Run evaluation for a single test case"""
    start_time = time.time()
    
    try:
        # Generate and validate transactions
        generator = TradingDataGenerator(test_case.params)
        transactions = generator.generate_transactions()
        is_valid = validate_transactions(transactions)
        
        # Get ground truth information
        ground_truth_info = generator.get_ground_truth_info()
        ground_truth_addresses = set()
        for group in generator.wash_groups:
            ground_truth_addresses.update(group)
        
        # Run analysis
        analysis = analyze_wash_trading(transactions, api_key)
        predicted_addresses = set(analysis.suspicious_addresses)
        
        # Calculate execution time
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Compute metrics
        metrics = compute_metrics(
            ground_truth_addresses,
            predicted_addresses,
            set(generator.addresses),
            execution_time,
            ground_truth_info,
            analysis.dict()
        )
        
        # Generate visualization
        graph_path = create_transaction_graph(transactions)
        
        return {
            "difficulty": test_case.difficulty.value,
            "seed": test_case.seed,
            "num_transactions": len(transactions),
            "is_valid": is_valid,
            "graph_path": str(graph_path),
            "execution_time": metrics.execution_time,
            "precision": metrics.precision,
            "recall": metrics.recall,
            "f1_score": metrics.f1_score,
            "true_positives": metrics.true_positives,
            "false_positives": metrics.false_positives,
            "false_negatives": metrics.false_negatives,
            "true_negatives": metrics.true_negatives,
            "ground_truth_info": metrics.ground_truth_info,
            "detection_info": metrics.detection_info
        }
        
    except Exception as e:
        logger.error(f"Error in evaluation: {str(e)}")
        raise

def save_results(results: List[Dict], output_dir: Path):
    """Save evaluation results to files"""
    # Save detailed results as JSON
    with open(output_dir / "detailed_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    # Create summary DataFrame
    summary_df = pd.DataFrame(results)
    summary_df.to_csv(output_dir / "summary_results.csv", index=False)
    
    # Generate LaTeX tables
    metrics_by_difficulty = summary_df.groupby('difficulty').agg({
        'precision': ['mean', 'std'],
        'recall': ['mean', 'std'],
        'f1_score': ['mean', 'std'],
        'execution_time': ['mean', 'std']
    }).round(3)
    
    with open(output_dir / "latex_tables.tex", "w") as f:
        f.write("% Detection Performance\n")
        f.write(metrics_by_difficulty.to_latex())
    
    # Print summary statistics
    logger.info("\nEvaluation Summary:")
    logger.info("==================")
    logger.info(f"Total test cases: {len(results)}")
    logger.info("\nMetrics by difficulty:")
    logger.info("\n" + str(metrics_by_difficulty))
    
    # Generate plots
    plot_metrics(results, output_dir)

def main():
    # Load environment variables
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set")
    
    # Setup output directory
    output_dir = setup_output_directory()
    logger.info(f"Results will be saved to: {output_dir}")
    
    # Generate test cases (4 cases per difficulty level)
    test_cases = generate_test_cases(4)
    logger.info(f"Generated {len(test_cases)} test cases")
    
    # Run evaluations in parallel
    results = []
    with concurrent.futures.ProcessPoolExecutor() as executor:
        future_to_case = {
            executor.submit(run_single_evaluation, case, api_key): case 
            for case in test_cases
        }
        
        for future in concurrent.futures.as_completed(future_to_case):
            case = future_to_case[future]
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Completed evaluation for {case.difficulty.value} case with seed {case.seed}")
            except Exception as e:
                logger.error(f"Error processing {case.difficulty.value} case with seed {case.seed}: {str(e)}")
    
    # Save and display results
    save_results(results, output_dir)

if __name__ == "__main__":
    main()

================
File: main.py
================
from washdetector.generator.models import GeneratorParams, WashCamouflageParams, WashCamouflageStrategy
from washdetector.generator.generator import TradingDataGenerator
from washdetector.utils.validation import validate_transactions
from washdetector.visualization.graph import create_transaction_graph
from washdetector.agent.wash_detector import analyze_wash_trading


def main(seed, camouflage: bool = False):
    if camouflage:
        params = GeneratorParams(
            seed=42,
            num_transactions=200,
            num_normal_traders=20,
            num_wash_groups=2,
            wash_group_sizes=(3, 4),
            wash_amounts=(10.0, 20.0),
            wash_tx_counts=(20, 30),
            time_span_days=30,
            num_market_makers=5,
            market_maker_activeness=0.6,
            camouflage=WashCamouflageParams(
                strategy=WashCamouflageStrategy.HYBRID,
                normal_trade_probability=0.3,
                volume_leakage=0.2,
                normal_connections_per_washer=5,
                wash_amount_variance=0.3,
                min_time_between_wash=1.0
            )
        )
    else:
        # starter test params
        # params = GeneratorParams(
        #     seed=seed,
        #     num_transactions=200,
        #     num_normal_traders=10,
        #     num_wash_groups=2,
        #     wash_group_sizes=(3, 4),
        #     wash_amounts=(10.0, 20.0),
        #     wash_tx_counts=(20, 30),
        #     time_span_days=30,
        # )
        
        # no camouflage, washers are identified
        params = GeneratorParams(
            seed=seed,
            num_transactions=300,
            num_normal_traders=100,
            num_wash_groups=1,
            wash_group_sizes=(4,),
            wash_amounts=(20.0,),
            wash_tx_counts=(30,),
            time_span_days=30,
        )

        # no camouflage, yes market makers, washers are identified
        params = GeneratorParams(
            seed=seed,
            num_transactions=300,
            num_normal_traders=100,
            num_wash_groups=1,
            wash_group_sizes=(4,),
            wash_amounts=(20.0,),
            wash_tx_counts=(30,),
            time_span_days=30,
            num_market_makers=5,
            market_maker_activeness=0.6,
        )

        params = GeneratorParams(
            seed=seed,
            num_transactions=300,
            num_normal_traders=100,
            num_wash_groups=1,
            wash_group_sizes=(4,),
            wash_amounts=(20.0,),
            wash_tx_counts=(30,),
            time_span_days=30,
            camouflage=WashCamouflageParams(
                strategy=WashCamouflageStrategy.HYBRID,
                normal_trade_probability=0.3,
                volume_leakage=0.2,
                normal_connections_per_washer=5,
                wash_amount_variance=0.3,
                min_time_between_wash=1.0
            )
        )

        params = GeneratorParams(
            seed=seed,
            num_transactions=1000,
            num_normal_traders=100,
            num_wash_groups=2,
            wash_group_sizes=(4, 7),
            wash_amounts=(20.0, 10.0),
            wash_tx_counts=(30, 20),
            time_span_days=30,
            num_market_makers=5,
            market_maker_activeness=0.6,
            camouflage=WashCamouflageParams(
                strategy=WashCamouflageStrategy.HYBRID,
                normal_trade_probability=0.3,
                volume_leakage=0.2,
                normal_connections_per_washer=5,
                wash_amount_variance=0.3,
                min_time_between_wash=1.0
            )
        )

    generator = TradingDataGenerator(params)
    transactions = generator.generate_transactions()
    is_valid = validate_transactions(transactions)
    
    print(f"Generated {len(transactions)} transactions")
    print(f"Valid sequence: {is_valid}")
    

    # print(transactions)
    analysis = analyze_wash_trading(transactions, OPENAI_API_KEY)

    #visualize 
    print("Creating transaction graph visualization...")
    graph_path = create_transaction_graph(transactions)
    print(f"Graph saved to: {graph_path}")
    
    return transactions, analysis

if __name__ == "__main__":
    from dotenv import load_dotenv
    import os

    load_dotenv()
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

    # seeds = [42, 123, 456]

    # for seed in seeds:
    #     print(f"\nRunning evaluation with seed: {seed}")
    #     transactions, analysis = main(seed, camouflage=True)
    #     print(analysis)
    
    transactions, analysis = main(1, False)
    print(analysis)

================
File: visualize_data.py
================
from washdetector.generator.models import GeneratorParams, WashCamouflageParams, WashCamouflageStrategy
from washdetector.generator.generator import TradingDataGenerator
from washdetector.utils.validation import validate_transactions
from washdetector.visualization.graph import create_transaction_graph

def main(camouflage: bool = False):
    if camouflage:
        params = GeneratorParams(
            seed=42,
            num_transactions=1000,
            num_normal_traders=50,
            num_wash_groups=2,
            wash_group_sizes=(3, 4),
            wash_amounts=(10.0, 20.0),
            wash_tx_counts=(20, 30),
            time_span_days=30,
            num_market_makers=5,
            market_maker_activeness=0.6,
            camouflage=WashCamouflageParams(
                strategy=WashCamouflageStrategy.HYBRID,
                normal_trade_probability=0.3,
                volume_leakage=0.2,
                normal_connections_per_washer=5,
                wash_amount_variance=0.3,
                min_time_between_wash=1.0
            )
        )
    else:
        params = GeneratorParams(
            seed=42,
            num_transactions=1000,
            num_normal_traders=50,
            num_wash_groups=2,
            wash_group_sizes=(3, 4),
            wash_amounts=(10.0, 20.0),
            wash_tx_counts=(20, 30),
            time_span_days=30,
        )

    generator = TradingDataGenerator(params)
    transactions = generator.generate_transactions()
    is_valid = validate_transactions(transactions)
    
    print(f"Generated {len(transactions)} transactions")
    print(f"Valid sequence: {is_valid}")
    
    #verify determinism
    gen2 = TradingDataGenerator(params)
    transactions2 = gen2.generate_transactions()
    assert transactions == transactions2, "Generator is not deterministic!"
    
    #visualize 
    print("Creating transaction graph visualization...")
    graph_path = create_transaction_graph(transactions)
    print(f"Graph saved to: {graph_path}")
    
    return transactions

if __name__ == "__main__":
    transactions = main(True)
